{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73fbb057-e3dc-49f0-b383-1eeb5c7eb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove from comments if you are running this for the first time\n",
    "# !pip install meteostat -U\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from meteostat import Stations, Hourly, Daily\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5689cbd6-c6a3-46c2-84ff-32fef068955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCovidDataFrame(cntry, savePath=\"/\", saveType=\"variable\"):\n",
    "    start_time = time.time()\n",
    "    # Print iterations progress\n",
    "    def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
    "        \"\"\"\n",
    "        Call in a loop to create terminal progress bar\n",
    "        @params:\n",
    "            iteration   - Required  : current iteration (Int)\n",
    "            total       - Required  : total iterations (Int)\n",
    "            prefix      - Optional  : prefix string (Str)\n",
    "            suffix      - Optional  : suffix string (Str)\n",
    "            decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "            length      - Optional  : character length of bar (Int)\n",
    "            fill        - Optional  : bar fill character (Str)\n",
    "            printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "        \"\"\"\n",
    "        percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "        filledLength = int(length * iteration // total)\n",
    "        bar = fill * filledLength + '-' * (length - filledLength)\n",
    "        print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "        # Print New Line on Complete\n",
    "        if iteration == total:\n",
    "            print()\n",
    "            \n",
    "    printProgressBar(0, 100, prefix = 'Confirmed, Deaths and Recovered data', suffix = 'Complete', length = 100)       \n",
    "    confirmed_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
    "    printProgressBar(33, 100, prefix = 'Confirmed, Deaths and Recovered data', suffix = 'Complete', length = 100)   \n",
    "    deaths_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "    printProgressBar(66, 100, prefix = 'Confirmed, Deaths and Recovered data', suffix = 'Complete', length = 100)   \n",
    "    recovered_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\n",
    "    printProgressBar(100, 100, prefix = 'Confirmed, Deaths and Recovered data', suffix = 'Complete', length = 100)   \n",
    "    \n",
    "    printProgressBar(0, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)   \n",
    "    confirmed_df = confirmed_df.replace(np.nan, '', regex=True)\n",
    "    deaths_df = deaths_df.replace(np.nan, '', regex=True)\n",
    "    recovered_df = recovered_df.replace(np.nan, '', regex=True)\n",
    "    printProgressBar(5, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    temp_cr = []\n",
    "    for i in range(len(confirmed_df.index)):\n",
    "        if (confirmed_df[\"Province/State\"][i] != ''):\n",
    "            temp_cr.append(\"{}/{}\".format(confirmed_df[\"Country/Region\"][i] , confirmed_df[\"Province/State\"][i]))\n",
    "        else:\n",
    "            temp_cr.append(confirmed_df[\"Country/Region\"][i])\n",
    "    confirmed_df[\"Country/Region\"] = temp_cr\n",
    "    \n",
    "    temp_cr = []\n",
    "    for i in range(len(deaths_df.index)):\n",
    "        if (deaths_df[\"Province/State\"][i] != ''):\n",
    "            temp_cr.append(\"{}/{}\".format(deaths_df[\"Country/Region\"][i] , deaths_df[\"Province/State\"][i]))\n",
    "        else:\n",
    "            temp_cr.append(deaths_df[\"Country/Region\"][i])\n",
    "    deaths_df[\"Country/Region\"] = temp_cr\n",
    "    \n",
    "    temp_cr = []\n",
    "    for i in range(len(recovered_df.index)):\n",
    "        if (recovered_df[\"Province/State\"][i] != ''):\n",
    "            temp_cr.append(\"{}/{}\".format(recovered_df[\"Country/Region\"][i] , recovered_df[\"Province/State\"][i]))\n",
    "        else:\n",
    "            temp_cr.append(recovered_df[\"Country/Region\"][i])\n",
    "    recovered_df[\"Country/Region\"] = temp_cr\n",
    "    \n",
    "    confirmed_df = confirmed_df.drop(labels = ['Province/State'], axis = 1)\n",
    "    deaths_df = deaths_df.drop(labels = ['Province/State'], axis = 1)\n",
    "    recovered_df = recovered_df.drop(labels = ['Province/State'], axis = 1)\n",
    "    printProgressBar(10, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    coordinates_df = confirmed_df[[\"Country/Region\", \"Lat\", \"Long\"]]\n",
    "    coordinates_df = coordinates_df.T\n",
    "    new_header = coordinates_df.iloc[0] #grab the first row for the header\n",
    "    coordinates_df = coordinates_df[1:] #take the data less the header row\n",
    "    coordinates_df.columns = new_header #set the header row as the df header\n",
    "    confirmed_df = confirmed_df.drop(labels = ['Lat','Long'], axis = 1)\n",
    "    deaths_df = deaths_df.drop(labels = ['Lat','Long'], axis = 1)\n",
    "    recovered_df = recovered_df.drop(labels = ['Lat','Long'], axis = 1)\n",
    "    confirmed_df = confirmed_df.T\n",
    "    deaths_df = deaths_df.T\n",
    "    recovered_df = recovered_df.T\n",
    "    new_header = confirmed_df.iloc[0] #grab the first row for the header\n",
    "    confirmed_df = confirmed_df[1:] #take the data less the header row\n",
    "    confirmed_df.columns = new_header #set the header row as the df header\n",
    "    confirmed_df.index = pd.to_datetime(confirmed_df.index)\n",
    "\n",
    "    new_header = deaths_df.iloc[0] #grab the first row for the header\n",
    "    deaths_df = deaths_df[1:] #take the data less the header row\n",
    "    deaths_df.columns = new_header #set the header row as the df header\n",
    "    deaths_df.index = pd.to_datetime(deaths_df.index)\n",
    "\n",
    "    new_header = recovered_df.iloc[0] #grab the first row for the header\n",
    "    recovered_df = recovered_df[1:] #take the data less the header row\n",
    "    recovered_df.columns = new_header #set the header row as the df header\n",
    "    recovered_df.index = pd.to_datetime(recovered_df.index)\n",
    "    printProgressBar(20, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    confirmed_df[\"Australia\"] = confirmed_df[\"Australia/Western Australia\"] +confirmed_df[\"Australia/Victoria\"] +confirmed_df[\"Australia/Tasmania\"] +confirmed_df[\"Australia/South Australia\"] +confirmed_df[\"Australia/Queensland\"] +confirmed_df[\"Australia/Northern Territory\"] +confirmed_df[\"Australia/New South Wales\"] +confirmed_df[\"Australia/Australian Capital Territory\"]\n",
    "    confirmed_df = confirmed_df.drop(labels = ['Australia/Australian Capital Territory','Australia/Western Australia','Australia/Victoria','Australia/Tasmania','Australia/South Australia','Australia/Queensland','Australia/Northern Territory','Australia/New South Wales'], axis = 1)\n",
    "    coordinates_df[\"Australia\"] = coordinates_df[\"Australia/Western Australia\"]\n",
    "    coordinates_df = coordinates_df.drop(labels = ['Australia/Australian Capital Territory','Australia/Western Australia','Australia/Victoria','Australia/Tasmania','Australia/South Australia','Australia/Queensland','Australia/Northern Territory','Australia/New South Wales'], axis = 1)\n",
    "    \n",
    "    \n",
    "    printProgressBar(30, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    confirmed_df[\"Canada\"] = confirmed_df[\"Canada/Alberta\"] + confirmed_df[\"Canada/British Columbia\"] + confirmed_df[\"Canada/Diamond Princess\"] + confirmed_df[\"Canada/Grand Princess\"] + confirmed_df[\"Canada/Manitoba\"] + confirmed_df[\"Canada/New Brunswick\"] + confirmed_df[\"Canada/Newfoundland and Labrador\"] + confirmed_df[\"Canada/Northwest Territories\"] + confirmed_df[\"Canada/Nova Scotia\"] + confirmed_df[\"Canada/Nunavut\"] + confirmed_df[\"Canada/Ontario\"] + confirmed_df[\"Canada/Prince Edward Island\"] + confirmed_df[\"Canada/Quebec\"] + confirmed_df[\"Canada/Repatriated Travellers\"] + confirmed_df[\"Canada/Saskatchewan\"] +confirmed_df[\"Canada/Yukon\"] \n",
    "\n",
    "    confirmed_df = confirmed_df.drop(labels = ['Canada/Alberta','Canada/Yukon','Canada/Saskatchewan','Canada/Repatriated Travellers','Canada/Quebec','Canada/Prince Edward Island','Canada/Ontario','Canada/Nunavut','Canada/Nova Scotia','Canada/Northwest Territories','Canada/Newfoundland and Labrador','Canada/New Brunswick','Canada/Manitoba','Canada/Grand Princess','Canada/Diamond Princess','Canada/British Columbia'], axis = 1)\n",
    "\n",
    "    \n",
    "    coordinates_df[\"Canada\"] = coordinates_df[\"Canada/British Columbia\"]\n",
    "    coordinates_df = coordinates_df.drop(labels = ['Canada/Alberta','Canada/Yukon','Canada/Saskatchewan','Canada/Repatriated Travellers','Canada/Quebec','Canada/Prince Edward Island','Canada/Ontario','Canada/Nunavut','Canada/Nova Scotia','Canada/Northwest Territories','Canada/Newfoundland and Labrador','Canada/New Brunswick','Canada/Manitoba','Canada/Grand Princess','Canada/Diamond Princess','Canada/British Columbia'], axis = 1)\n",
    "\n",
    "\n",
    "    confirmed_df = confirmed_df.drop(labels = ['United Kingdom/Anguilla','United Kingdom/Saint Helena, Ascension and Tristan da Cunha','United Kingdom/Montserrat','United Kingdom/Isle of Man','United Kingdom/Gibraltar','United Kingdom/Falkland Islands (Malvinas)','United Kingdom/Channel Islands','United Kingdom/Cayman Islands','United Kingdom/British Virgin Islands','United Kingdom/Bermuda'], axis = 1)\n",
    "    \n",
    "\n",
    "    confirmed_df = confirmed_df.drop(labels = ['France/French Guiana','France/Wallis and Futuna','France/St Martin','France/Saint Pierre and Miquelon','France/Saint Barthelemy','France/Reunion','France/New Caledonia','France/Mayotte','France/Martinique','France/Guadeloupe','France/Guadeloupe','France/French Polynesia'], axis = 1)\n",
    "    \n",
    "\n",
    "    confirmed_df[\"Democratic Republic of Congo\"] = confirmed_df[\"Congo (Brazzaville)\"] + confirmed_df[\"Congo (Kinshasa)\"] \n",
    "\n",
    "    confirmed_df = confirmed_df.drop(labels = ['Congo (Brazzaville)'], axis = 1)\n",
    "    confirmed_df = confirmed_df.drop(labels = ['Congo (Kinshasa)'], axis = 1)\n",
    "    \n",
    "    coordinates_df[\"Democratic Republic of Congo\"] = coordinates_df[\"Congo (Brazzaville)\"]\n",
    "    coordinates_df = coordinates_df.drop(labels = ['Congo (Brazzaville)'], axis = 1)\n",
    "    coordinates_df = coordinates_df.drop(labels = ['Congo (Kinshasa)'], axis = 1)\n",
    "    \n",
    "    printProgressBar(40, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    confirmed_df[\"China\"] = confirmed_df['China/Anhui'] +confirmed_df['China/Beijing'] +confirmed_df['China/Chongqing'] +confirmed_df['China/Fujian'] +confirmed_df['China/Gansu'] +confirmed_df['China/Guangdong'] +confirmed_df['China/Guangxi'] +confirmed_df['China/Guizhou'] +confirmed_df['China/Hainan'] +confirmed_df['China/Hebei'] +confirmed_df['China/Heilongjiang'] +confirmed_df['China/Henan'] +confirmed_df['China/Hong Kong'] +confirmed_df['China/Hubei'] +confirmed_df['China/Hunan'] +confirmed_df['China/Inner Mongolia'] +confirmed_df['China/Jiangsu'] +confirmed_df['China/Jiangxi'] +confirmed_df['China/Jilin'] +confirmed_df['China/Liaoning'] +confirmed_df['China/Macau'] +confirmed_df['China/Ningxia'] +confirmed_df['China/Qinghai'] +confirmed_df['China/Shaanxi'] +confirmed_df['China/Shandong'] +confirmed_df['China/Shanghai'] +confirmed_df['China/Shanxi'] +confirmed_df['China/Sichuan'] +confirmed_df['China/Tianjin'] +confirmed_df['China/Tibet'] +confirmed_df['China/Xinjiang'] +confirmed_df['China/Yunnan'] +confirmed_df['China/Zhejiang']\n",
    "\n",
    "    confirmed_df = confirmed_df.drop(labels = ['China/Anhui','China/Zhejiang','China/Yunnan','China/Xinjiang','China/Tibet','China/Tianjin','China/Sichuan','China/Shanxi','China/Shanghai','China/Shandong','China/Shaanxi','China/Qinghai','China/Ningxia','China/Macau','China/Liaoning','China/Jilin','China/Jiangxi','China/Jiangsu','China/Inner Mongolia','China/Hunan','China/Hubei','China/Hong Kong','China/Henan','China/Heilongjiang','China/Hebei','China/Hainan','China/Guizhou','China/Guangxi','China/Guangdong','China/Gansu','China/Fujian','China/Chongqing','China/Beijing'], axis = 1)\n",
    "    \n",
    "    coordinates_df[\"China\"] = coordinates_df[\"China/Beijing\"]\n",
    "    coordinates_df = coordinates_df.drop(labels = ['China/Anhui','China/Zhejiang','China/Yunnan','China/Xinjiang','China/Tibet','China/Tianjin','China/Sichuan','China/Shanxi','China/Shanghai','China/Shandong','China/Shaanxi','China/Qinghai','China/Ningxia','China/Macau','China/Liaoning','China/Jilin','China/Jiangxi','China/Jiangsu','China/Inner Mongolia','China/Hunan','China/Hubei','China/Hong Kong','China/Henan','China/Heilongjiang','China/Hebei','China/Hainan','China/Guizhou','China/Guangxi','China/Guangdong','China/Gansu','China/Fujian','China/Chongqing','China/Beijing'], axis = 1)\n",
    "    \n",
    "    confirmed_df = confirmed_df.drop(labels = ['Netherlands/Aruba','Netherlands/Sint Maarten','Netherlands/Curacao','Netherlands/Bonaire, Sint Eustatius and Saba'], axis = 1)\n",
    "    \n",
    "\n",
    "    confirmed_df = confirmed_df.drop(labels = ['Denmark/Faroe Islands','Denmark/Greenland'], axis = 1)\n",
    "    \n",
    "    printProgressBar(50, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    #deaths_df\n",
    "    deaths_df[\"Australia\"] = deaths_df[\"Australia/Western Australia\"] +deaths_df[\"Australia/Victoria\"] +deaths_df[\"Australia/Tasmania\"] +deaths_df[\"Australia/South Australia\"] +deaths_df[\"Australia/Queensland\"] +deaths_df[\"Australia/Northern Territory\"] +deaths_df[\"Australia/New South Wales\"] +deaths_df[\"Australia/Australian Capital Territory\"]\n",
    "\n",
    "    deaths_df = deaths_df.drop(labels = ['Australia/Australian Capital Territory','Australia/Western Australia','Australia/Victoria','Australia/Tasmania','Australia/South Australia','Australia/Queensland','Australia/Northern Territory','Australia/New South Wales'], axis = 1)\n",
    "    \n",
    "\n",
    "    deaths_df[\"Canada\"] = deaths_df[\"Canada/Alberta\"] + deaths_df[\"Canada/British Columbia\"] + deaths_df[\"Canada/Diamond Princess\"] + deaths_df[\"Canada/Grand Princess\"] + deaths_df[\"Canada/Manitoba\"] + deaths_df[\"Canada/New Brunswick\"] + deaths_df[\"Canada/Newfoundland and Labrador\"] + deaths_df[\"Canada/Northwest Territories\"] + deaths_df[\"Canada/Nova Scotia\"] + deaths_df[\"Canada/Nunavut\"] + deaths_df[\"Canada/Ontario\"] + deaths_df[\"Canada/Prince Edward Island\"] + deaths_df[\"Canada/Quebec\"] + deaths_df[\"Canada/Repatriated Travellers\"] + deaths_df[\"Canada/Saskatchewan\"] +deaths_df[\"Canada/Yukon\"] \n",
    "\n",
    "    deaths_df = deaths_df.drop(labels = ['Canada/Alberta','Canada/Yukon','Canada/Saskatchewan','Canada/Repatriated Travellers','Canada/Quebec','Canada/Prince Edward Island','Canada/Ontario','Canada/Nunavut','Canada/Nova Scotia','Canada/Northwest Territories','Canada/Newfoundland and Labrador','Canada/New Brunswick','Canada/Manitoba','Canada/Grand Princess','Canada/Diamond Princess','Canada/British Columbia'], axis = 1)\n",
    "\n",
    "\n",
    "    deaths_df = deaths_df.drop(labels = ['United Kingdom/Anguilla','United Kingdom/Saint Helena, Ascension and Tristan da Cunha','United Kingdom/Montserrat','United Kingdom/Isle of Man','United Kingdom/Gibraltar','United Kingdom/Falkland Islands (Malvinas)','United Kingdom/Channel Islands','United Kingdom/Cayman Islands','United Kingdom/British Virgin Islands','United Kingdom/Bermuda'], axis = 1)\n",
    "    \n",
    "\n",
    "    deaths_df = deaths_df.drop(labels = ['France/French Guiana','France/Wallis and Futuna','France/St Martin','France/Saint Pierre and Miquelon','France/Saint Barthelemy','France/Reunion','France/New Caledonia','France/Mayotte','France/Martinique','France/Guadeloupe','France/Guadeloupe','France/French Polynesia'], axis = 1)\n",
    "    \n",
    "\n",
    "    deaths_df[\"Congo\"] = deaths_df[\"Congo (Brazzaville)\"] + deaths_df[\"Congo (Kinshasa)\"] \n",
    "\n",
    "    deaths_df = deaths_df.drop(labels = ['Congo (Brazzaville)'], axis = 1)\n",
    "    deaths_df = deaths_df.drop(labels = ['Congo (Kinshasa)'], axis = 1)\n",
    "\n",
    "    printProgressBar(60, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    deaths_df[\"China\"] = deaths_df['China/Anhui'] +deaths_df['China/Beijing'] +deaths_df['China/Chongqing'] +deaths_df['China/Fujian'] +deaths_df['China/Gansu'] +deaths_df['China/Guangdong'] +deaths_df['China/Guangxi'] +deaths_df['China/Guizhou'] +deaths_df['China/Hainan'] +deaths_df['China/Hebei'] +deaths_df['China/Heilongjiang'] +deaths_df['China/Henan'] +deaths_df['China/Hong Kong'] +deaths_df['China/Hubei'] +deaths_df['China/Hunan'] +deaths_df['China/Inner Mongolia'] +deaths_df['China/Jiangsu'] +deaths_df['China/Jiangxi'] +deaths_df['China/Jilin'] +deaths_df['China/Liaoning'] +deaths_df['China/Macau'] +deaths_df['China/Ningxia'] +deaths_df['China/Qinghai'] +deaths_df['China/Shaanxi'] +deaths_df['China/Shandong'] +deaths_df['China/Shanghai'] +deaths_df['China/Shanxi'] +deaths_df['China/Sichuan'] +deaths_df['China/Tianjin'] +deaths_df['China/Tibet'] +deaths_df['China/Xinjiang'] +deaths_df['China/Yunnan'] +deaths_df['China/Zhejiang']\n",
    "\n",
    "    deaths_df = deaths_df.drop(labels = ['China/Anhui','China/Zhejiang','China/Yunnan','China/Xinjiang','China/Tibet','China/Tianjin','China/Sichuan','China/Shanxi','China/Shanghai','China/Shandong','China/Shaanxi','China/Qinghai','China/Ningxia','China/Macau','China/Liaoning','China/Jilin','China/Jiangxi','China/Jiangsu','China/Inner Mongolia','China/Hunan','China/Hubei','China/Hong Kong','China/Henan','China/Heilongjiang','China/Hebei','China/Hainan','China/Guizhou','China/Guangxi','China/Guangdong','China/Gansu','China/Fujian','China/Chongqing','China/Beijing'], axis = 1)\n",
    "    \n",
    "\n",
    "    deaths_df = deaths_df.drop(labels = ['Netherlands/Aruba','Netherlands/Sint Maarten','Netherlands/Curacao','Netherlands/Bonaire, Sint Eustatius and Saba'], axis = 1)\n",
    "    \n",
    "\n",
    "    deaths_df = deaths_df.drop(labels = ['Denmark/Faroe Islands'], axis = 1)\n",
    "    deaths_df = deaths_df.drop(labels = ['Denmark/Greenland'], axis = 1)\n",
    "\n",
    "    printProgressBar(70, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    #recovered_df\n",
    "    recovered_df[\"Australia\"] = recovered_df[\"Australia/Western Australia\"] +recovered_df[\"Australia/Victoria\"] +recovered_df[\"Australia/Tasmania\"] +recovered_df[\"Australia/South Australia\"] +recovered_df[\"Australia/Queensland\"] +recovered_df[\"Australia/Northern Territory\"] +recovered_df[\"Australia/New South Wales\"] +recovered_df[\"Australia/Australian Capital Territory\"]\n",
    "\n",
    "    recovered_df = recovered_df.drop(labels = ['Australia/Australian Capital Territory','Australia/Western Australia','Australia/Victoria','Australia/Tasmania','Australia/South Australia','Australia/Queensland','Australia/Northern Territory','Australia/New South Wales'], axis = 1)\n",
    "    \n",
    "\n",
    "\n",
    "    recovered_df = recovered_df.drop(labels = ['United Kingdom/Anguilla','United Kingdom/Saint Helena, Ascension and Tristan da Cunha','United Kingdom/Montserrat','United Kingdom/Isle of Man','United Kingdom/Gibraltar','United Kingdom/Falkland Islands (Malvinas)','United Kingdom/Channel Islands','United Kingdom/Cayman Islands','United Kingdom/British Virgin Islands','United Kingdom/Bermuda'], axis = 1)\n",
    "    \n",
    "    recovered_df = recovered_df.drop(labels = ['France/French Guiana','France/Wallis and Futuna','France/St Martin','France/Saint Pierre and Miquelon','France/Saint Barthelemy','France/Reunion','France/New Caledonia','France/Mayotte','France/Martinique','France/Guadeloupe','France/Guadeloupe','France/French Polynesia'], axis = 1)\n",
    "    \n",
    "\n",
    "    recovered_df[\"Congo\"] = recovered_df[\"Congo (Brazzaville)\"] + recovered_df[\"Congo (Kinshasa)\"] \n",
    "\n",
    "    recovered_df = recovered_df.drop(labels = ['Congo (Brazzaville)'], axis = 1)\n",
    "    recovered_df = recovered_df.drop(labels = ['Congo (Kinshasa)'], axis = 1)\n",
    "\n",
    "    printProgressBar(80, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    recovered_df[\"China\"] = recovered_df['China/Anhui'] +recovered_df['China/Beijing'] +recovered_df['China/Chongqing'] +recovered_df['China/Fujian'] +recovered_df['China/Gansu'] +recovered_df['China/Guangdong'] +recovered_df['China/Guangxi'] +recovered_df['China/Guizhou'] +recovered_df['China/Hainan'] +recovered_df['China/Hebei'] +recovered_df['China/Heilongjiang'] +recovered_df['China/Henan'] +recovered_df['China/Hong Kong'] +recovered_df['China/Hubei'] +recovered_df['China/Hunan'] +recovered_df['China/Inner Mongolia'] +recovered_df['China/Jiangsu'] +recovered_df['China/Jiangxi'] +recovered_df['China/Jilin'] +recovered_df['China/Liaoning'] +recovered_df['China/Macau'] +recovered_df['China/Ningxia'] +recovered_df['China/Qinghai'] +recovered_df['China/Shaanxi'] +recovered_df['China/Shandong'] +recovered_df['China/Shanghai'] +recovered_df['China/Shanxi'] +recovered_df['China/Sichuan'] +recovered_df['China/Tianjin'] +recovered_df['China/Tibet'] +recovered_df['China/Xinjiang'] +recovered_df['China/Yunnan'] +recovered_df['China/Zhejiang']\n",
    "\n",
    "    recovered_df = recovered_df.drop(labels = ['China/Anhui','China/Zhejiang','China/Yunnan','China/Xinjiang','China/Tibet','China/Tianjin','China/Sichuan','China/Shanxi','China/Shanghai','China/Shandong','China/Shaanxi','China/Qinghai','China/Ningxia','China/Macau','China/Liaoning','China/Jilin','China/Jiangxi','China/Jiangsu','China/Inner Mongolia','China/Hunan','China/Hubei','China/Hong Kong','China/Henan','China/Heilongjiang','China/Hebei','China/Hainan','China/Guizhou','China/Guangxi','China/Guangdong','China/Gansu','China/Fujian','China/Chongqing','China/Beijing'], axis = 1)\n",
    "    \n",
    "\n",
    "    recovered_df = recovered_df.drop(labels = ['Netherlands/Aruba','Netherlands/Sint Maarten','Netherlands/Curacao','Netherlands/Bonaire, Sint Eustatius and Saba'], axis = 1)\n",
    "    \n",
    "\n",
    "    recovered_df = recovered_df.drop(labels = ['Denmark/Faroe Islands'], axis = 1)\n",
    "    recovered_df = recovered_df.drop(labels = ['Denmark/Greenland'], axis = 1)\n",
    "    \n",
    "    printProgressBar(83, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    confirmed_df = confirmed_df.drop(labels = ['Afghanistan','Algeria','Angola','Antigua and Barbuda','Azerbaijan','Bahamas','Barbados','Belize','Benin','Botswana','Brunei','Burkina Faso','Burma','Burundi','Cabo Verde','Cambodia','Cameroon','Central African Republic','Chad','Comoros','Diamond Princess','Djibouti','Egypt','Equatorial Guinea','Eritrea','Eswatini','Gabon','Georgia','Grenada','Guinea','Guinea-Bissau','Guyana','Haiti','Holy See','Honduras','Korea, South','Kosovo','Kyrgyzstan','Laos','Lebanon','Lesotho','Liberia','Liechtenstein','MS Zaandam','Mali','Marshall Islands','Mauritius','Micronesia','Moldova','Monaco','Montenegro','Nicaragua','Papua New Guinea','Saint Kitts and Nevis','Saint Lucia','Saint Vincent and the Grenadines','Samoa','San Marino','Sao Tome and Principe','Seychelles','Sierra Leone','Solomon Islands','Somalia','Suriname','Syria','Taiwan*','Tajikistan','Tanzania','Timor-Leste','United Kingdom/Turks and Caicos Islands','Uzbekistan','Vanuatu','Venezuela','West Bank and Gaza','Yemen'], axis = 1)\n",
    "    \n",
    "    #deaths_df\n",
    "    deaths_df = deaths_df.drop(labels = ['Afghanistan','Algeria','Angola','Antigua and Barbuda','Azerbaijan','Bahamas','Barbados','Belize','Benin','Botswana','Brunei','Burkina Faso','Burma','Burundi','Cabo Verde','Cambodia','Cameroon','Central African Republic','Chad','Comoros','Diamond Princess','Djibouti','Egypt','Equatorial Guinea','Eritrea','Eswatini','Gabon','Georgia','Grenada','Guinea','Guinea-Bissau','Guyana','Haiti','Holy See','Honduras','Korea, South','Kosovo','Kyrgyzstan','Laos','Lebanon','Lesotho','Liberia','Liechtenstein','MS Zaandam','Mali','Marshall Islands','Mauritius','Micronesia','Moldova','Monaco','Montenegro','Nicaragua','Papua New Guinea','Saint Kitts and Nevis','Saint Lucia','Saint Vincent and the Grenadines','Samoa','San Marino','Sao Tome and Principe','Seychelles','Sierra Leone','Solomon Islands','Somalia','Suriname','Syria','Taiwan*','Tajikistan','Tanzania','Timor-Leste','United Kingdom/Turks and Caicos Islands','Uzbekistan','Vanuatu','Venezuela','West Bank and Gaza','Yemen'], axis = 1)\n",
    "\n",
    "    printProgressBar(90, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    #recovered_df\n",
    "    recovered_df = recovered_df.drop(labels = ['Afghanistan','Algeria','Angola','Antigua and Barbuda','Azerbaijan','Bahamas','Barbados','Belize','Benin','Botswana','Brunei','Burkina Faso','Burma','Burundi','Cabo Verde','Cambodia','Cameroon','Central African Republic','Chad','Comoros','Diamond Princess','Djibouti','Egypt','Equatorial Guinea','Eritrea','Eswatini','Gabon','Georgia','Grenada','Guinea','Guinea-Bissau','Guyana','Haiti','Holy See','Honduras','Korea, South','Kosovo','Kyrgyzstan','Laos','Lebanon','Lesotho','Liberia','Liechtenstein','MS Zaandam','Mali','Marshall Islands','Mauritius','Micronesia','Moldova','Monaco','Montenegro','Nicaragua','Papua New Guinea','Saint Kitts and Nevis','Saint Lucia','Saint Vincent and the Grenadines','Samoa','San Marino','Sao Tome and Principe','Seychelles','Sierra Leone','Solomon Islands','Somalia','Suriname','Syria','Taiwan*','Tajikistan','Tanzania','Timor-Leste','United Kingdom/Turks and Caicos Islands','Uzbekistan','Vanuatu','Venezuela','West Bank and Gaza','Yemen'], axis = 1)\n",
    "    \n",
    "    confirmed_df[\"United States\"] = confirmed_df[\"US\"]\n",
    "    confirmed_df = confirmed_df.drop(labels = ['US'], axis = 1)\n",
    "    \n",
    "    recovered_df[\"United States\"] = recovered_df[\"US\"]\n",
    "    recovered_df = recovered_df.drop(labels = ['US'], axis = 1)\n",
    "    \n",
    "    deaths_df[\"United States\"] = deaths_df[\"US\"]\n",
    "    deaths_df = deaths_df.drop(labels = ['US'], axis = 1)\n",
    "    \n",
    "    coordinates_df[\"United States\"] = coordinates_df[\"US\"]\n",
    "    coordinates_df = coordinates_df.drop(labels = ['US'], axis = 1)\n",
    "    printProgressBar(100, 100, prefix = 'Preparing data                      ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    \n",
    "    confirmed_df = confirmed_df.drop(labels = ['Armenia', 'Czechia', 'Maldives', 'North Macedonia', 'Slovakia'], axis = 1)\n",
    "    deaths_df = deaths_df.drop(labels = ['Armenia', 'Czechia', 'Maldives', 'North Macedonia', 'Slovakia'], axis = 1)\n",
    "    recovered_df = recovered_df.drop(labels = ['Armenia', 'Czechia', 'Maldives', 'North Macedonia', 'Slovakia'], axis = 1)\n",
    "    if cntry not in confirmed_df.columns:\n",
    "        print(\"Country: {} not available. Please provide one of the ollowing countries:\".format(cntry))\n",
    "        print(confirmed_df.columns.tolist())\n",
    "        return confirmed_df.columns.tolist()\n",
    "    \n",
    "    printProgressBar(0, 100, prefix = 'Calculating Daily data              ', suffix = 'Complete', length = 100)  \n",
    "    \n",
    "    confirmed_daily_df = pd.DataFrame()\n",
    "    deaths_daily_df = pd.DataFrame()\n",
    "    recovered_daily_df = pd.DataFrame()\n",
    "    confirmed_daily = []\n",
    "    deaths_daily = []\n",
    "    recovered_daily = []\n",
    "    \n",
    "    printProgressBar(25, 100, prefix = 'Calculating Daily data              ', suffix = 'Complete', length = 100) \n",
    "    \n",
    "    for j in confirmed_df.keys():\n",
    "        for i in range(confirmed_df.shape[0]):\n",
    "            if (i > 0):\n",
    "                confirmed_daily.append(confirmed_df[j][i] - confirmed_df[j][i-1])\n",
    "            else:\n",
    "                confirmed_daily.append(confirmed_df[j][i])\n",
    "        confirmed_daily_df[j] = confirmed_daily\n",
    "        confirmed_daily = []\n",
    "    confirmed_daily_df.index = confirmed_df.index\n",
    "    \n",
    "    printProgressBar(50, 100, prefix = 'Calculating Daily data              ', suffix = 'Complete', length = 100) \n",
    "\n",
    "    for j in deaths_df.keys():\n",
    "        for i in range(deaths_df.shape[0]):\n",
    "            if (i > 0):\n",
    "                deaths_daily.append(deaths_df[j][i] - deaths_df[j][i-1])\n",
    "            else:\n",
    "                deaths_daily.append(deaths_df[j][i])\n",
    "        deaths_daily_df[j] = deaths_daily\n",
    "        deaths_daily = []\n",
    "    deaths_daily_df.index = deaths_df.index\n",
    "    \n",
    "    printProgressBar(75, 100, prefix = 'Calculating Daily data              ', suffix = 'Complete', length = 100) \n",
    "\n",
    "    for j in recovered_df.keys():\n",
    "        for i in range(recovered_df.shape[0]):\n",
    "            if (i > 0):\n",
    "                value = recovered_df[j][i] - recovered_df[j][i-1]\n",
    "                if (value < 0):\n",
    "                    value = 0\n",
    "                recovered_daily.append(value)\n",
    "            else:\n",
    "                recovered_daily.append(recovered_df[j][i])\n",
    "        recovered_daily_df[j] = recovered_daily\n",
    "        recovered_daily = []\n",
    "    recovered_daily_df.index = recovered_df.index  \n",
    "    \n",
    "    printProgressBar(100, 100, prefix = 'Calculating Daily data              ', suffix = 'Complete', length = 100) \n",
    "  \n",
    "    \n",
    "    printProgressBar(0, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    Stations._types = {\n",
    "        'id': 'object',\n",
    "        'name': 'object',\n",
    "        'country': 'object',\n",
    "        'region': 'object',\n",
    "        'wmo': 'object',\n",
    "        'icao': 'object',\n",
    "        'latitude': 'float64',\n",
    "        'longitude': 'float64',\n",
    "        'elevation': 'float64',\n",
    "        'timezone': 'object'\n",
    "    }\n",
    "\n",
    "    stations = Stations()\n",
    "    stations = stations.nearby(coordinates_df[cntry][\"Lat\"], coordinates_df[cntry][\"Long\"])\n",
    "    station = stations.fetch(6)\n",
    "\n",
    "    printProgressBar(5, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "#     data = Hourly(station, start = confirmed_df.index[0].to_pydatetime() - datetime.timedelta(days=14), end = confirmed_df.index[confirmed_df.index.shape[0]-1].to_pydatetime() + datetime.timedelta(days=1))\n",
    "    \n",
    "#     data = data.fetch()\n",
    "\n",
    "    for key in range(0,6):\n",
    "        data = Hourly(station[station.index == station.index[key]], start = confirmed_df.index[0].to_pydatetime() - datetime.timedelta(days=14), end = confirmed_df.index[confirmed_df.index.shape[0]-1].to_pydatetime() + datetime.timedelta(days=1))\n",
    "        data = data.fetch()\n",
    "        if(data.shape[0] > 0):\n",
    "            break\n",
    "        else:\n",
    "            print('Weather data not available. Searching another weather station...')\n",
    "\n",
    "    data = data.drop(labels = ['dwpt','prcp','snow','wdir','wpgt','pres','tsun','coco'], axis = 1)\n",
    "    size = data.index.shape[0]-1\n",
    "    size = size/24\n",
    "    printProgressBar(10, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    date_rng = pd.date_range(start=confirmed_df.index[0].to_pydatetime() - datetime.timedelta(days=14), end=confirmed_df.index[0].to_pydatetime() + datetime.timedelta(days=size-14), freq='D')\n",
    "    avg_temp_daily_df = pd.DataFrame()\n",
    "    temp = []\n",
    "    size = int(size)\n",
    "    printProgressBar(15, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    for i in range(size):\n",
    "        temp.append(data.iloc[int(i*24):int(i*24)+24,0:3]['temp'].sum()/24)\n",
    "    avg_temp_daily_df[\"Avg Temp\"] = temp\n",
    "    avg_temp_daily_df.index = date_rng[0:size]\n",
    "    printProgressBar(20, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    temp = []\n",
    "    for i in range(size):\n",
    "        temp.append(data.iloc[int(i*24):int(i*24)+24,0:2]['rhum'].sum()/24)\n",
    "    avg_temp_daily_df[\"Avg Humidity\"] = temp\n",
    "    printProgressBar(25, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    temp = []\n",
    "    for i in range(size):\n",
    "        temp.append(data.iloc[int(i*24):int(i*24)+24,2:3]['wspd'].sum()/24)\n",
    "    avg_temp_daily_df[\"Avg Wind Speed\"] = temp\n",
    "    printProgressBar(30, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    end_rslt_df = pd.DataFrame()\n",
    "    \n",
    "    temp = []\n",
    "    for i in range(avg_temp_daily_df.index[:-14].shape[0]):\n",
    "        temp.append(avg_temp_daily_df.iloc[0+i:7+i,0:1]['Avg Temp'].sum()/7)\n",
    "    end_rslt_df['14_Day_Week_Average_Temp'] = temp\n",
    "    printProgressBar(60, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    temp = []\n",
    "    for i in range(avg_temp_daily_df.index[:-14].shape[0]):\n",
    "        temp.append(avg_temp_daily_df.iloc[0+i:7+i,0:4]['Avg Wind Speed'].sum()/7)\n",
    "    end_rslt_df['14_Day_Week_Average_Wind'] = temp\n",
    "    printProgressBar(80, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    temp = []\n",
    "    for i in range(avg_temp_daily_df.index[:-14].shape[0]):\n",
    "        temp.append(avg_temp_daily_df.iloc[0+i:7+i,0:4]['Avg Humidity'].sum()/7)\n",
    "    printProgressBar(90, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "    end_rslt_df['14_Day_Week_Average_Humidity'] = temp\n",
    "\n",
    "    end_rslt_df.index = pd.date_range(start=data.index[13].to_pydatetime().date(), end=data.index[13].to_pydatetime().date() + datetime.timedelta(days=size-15), freq='D')\n",
    "    end_rslt_df.index = pd.to_datetime(end_rslt_df.index)\n",
    "    printProgressBar(100, 100, prefix = 'Gathering Weather data              ', suffix = 'Complete', length = 100) \n",
    "\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    temp_df['Cumulative Confirmed'] = confirmed_df[cntry]\n",
    "    end_rslt_df = end_rslt_df.join(temp_df['Cumulative Confirmed'], how='outer')\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['Cumulative Deaths'] = deaths_df[cntry]\n",
    "    end_rslt_df = end_rslt_df.join(temp_df['Cumulative Deaths'], how='outer')\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['Cumulative Recovered'] = recovered_df[cntry]\n",
    "    end_rslt_df = end_rslt_df.join(temp_df['Cumulative Recovered'], how='outer')\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['Daily Confirmed'] = confirmed_daily_df[cntry]\n",
    "    end_rslt_df = end_rslt_df.join(temp_df['Daily Confirmed'], how='outer')\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['Daily Deaths'] = deaths_daily_df[cntry]\n",
    "    end_rslt_df = end_rslt_df.join(temp_df['Daily Deaths'], how='outer')\n",
    "    \n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['Daily Recovered'] = recovered_daily_df[cntry]\n",
    "    end_rslt_df = end_rslt_df.join(temp_df['Daily Recovered'], how='outer')\n",
    "    \n",
    "    \n",
    "    avg_temp_daily_df.index = pd.to_datetime(avg_temp_daily_df.index)\n",
    "    end_rslt_df = end_rslt_df.join(avg_temp_daily_df, how='outer')\n",
    "\n",
    "    printProgressBar(0, 100, prefix = 'Tests and Vaccination data          ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    testing_full_df = pd.read_csv('https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/testing/covid-testing-all-observations.csv')\n",
    "\n",
    "    testing_full_df = testing_full_df[testing_full_df.Entity != 'Canada - people tested']\n",
    "    testing_full_df = testing_full_df[testing_full_df.Entity != 'Argentina - people tested']\n",
    "    testing_full_df = testing_full_df[testing_full_df.Entity != 'Italy - people tested']\n",
    "    testing_full_df = testing_full_df[testing_full_df.Entity != 'Poland - people tested']\n",
    "\n",
    "    printProgressBar(15, 100, prefix = 'Tests and Vaccination data          ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    testing_full_df = testing_full_df[testing_full_df[\"Entity\"].isin(['{} - tests performed'.format(cntry), '{} - samples tested'.format(cntry),'{} - people tested'.format(cntry),'{} - units unclear'.format(cntry)])].fillna(0)\n",
    "    testing_full_df.index = testing_full_df.Date\n",
    "    testing_full_df = testing_full_df.drop(labels = ['Date','Source URL','Source label','Entity','ISO code','Notes'], axis = 1)\n",
    "    testing_full_df.index = pd.to_datetime(testing_full_df.index)\n",
    "\n",
    "    printProgressBar(30, 100, prefix = 'Tests and Vaccination data          ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    testing_full_df['positive_percent'] = (end_rslt_df['Daily Confirmed'] / testing_full_df['Daily change in cumulative total'])*100\n",
    "    testing_full_df['positive_percent'] = testing_full_df['positive_percent'].fillna(0)\n",
    "    testing_full_df['positive_percent'] = testing_full_df['positive_percent'].replace(np.inf, np.nan).interpolate()\n",
    "\n",
    "    printProgressBar(40, 100, prefix = 'Tests and Vaccination data          ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    testing_full_df = testing_full_df.rename(columns={\"Daily change in cumulative total\": \"Daily_Tests\", \"Cumulative total\": \"Cumulative_Tests_Total\", \"Cumulative total per thousand\":\"Cumulative_Total_Tests_Per_Thousand\",\"Daily change in cumulative total per thousand\":\"Daily_Change_Cumulative_Total_Tests_Per_Thousand\",\"7-day smoothed daily change\":\"7_Day_Smoothed_Test_Daily_Change\",\"7-day smoothed daily change per thousand\":\"7_Day_Smoothed_Test_Daily_Change_Per_Thousand\",\"Short-term positive rate\":\"Test_Short_Term_Positive_Rate\",\"Short-term tests per case\":\"Short_Term_Tests_Per_Case\"})\n",
    "\n",
    "    resultsS = testing_full_df.join(end_rslt_df, how='outer').interpolate()\n",
    "    resultsS[\"Daily_Test_Positivity_ewm_03\"] = resultsS[\"positive_percent\"].ewm(alpha=0.3).mean()\n",
    "    resultsS[\"Daily_Test_Positivity_ewm_05\"] = resultsS[\"positive_percent\"].ewm(alpha=0.5).mean()\n",
    "    resultsS[\"Daily_Test_Positivity_ewm_07\"] = resultsS[\"positive_percent\"].ewm(alpha=0.7).mean()\n",
    "\n",
    "    printProgressBar(60, 100, prefix = 'Tests and Vaccination data          ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    vaccinations_full_df = pd.read_csv('https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv')\n",
    "    vaccinations_full_df = vaccinations_full_df[vaccinations_full_df[\"location\"] == cntry].fillna(0)\n",
    "    vaccinations_full_df.index = vaccinations_full_df.date\n",
    "    vaccinations_full_df = vaccinations_full_df.drop(labels = ['date','iso_code','location','total_boosters', 'total_boosters_per_hundred'], axis = 1)\n",
    "    vaccinations_full_df.index = pd.to_datetime(vaccinations_full_df.index)\n",
    "\n",
    "    printProgressBar(90, 100, prefix = 'Tests and Vaccination data          ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    vaccinations_full_df = vaccinations_full_df.join(resultsS, how='outer').interpolate()\n",
    "    \n",
    "    \n",
    "    \n",
    "    info_full_df = pd.read_csv('https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv')\n",
    "    # temp = pd.unique(testing_full_df[\"location\"])\n",
    "    info_full_df = info_full_df[info_full_df[\"location\"] == cntry].fillna(0)\n",
    "    info_full_df.index = info_full_df.date\n",
    "    info_full_df.index = pd.to_datetime(info_full_df.index)\n",
    "    info_full_df = info_full_df.drop(labels = ['iso_code', 'continent', 'location', 'date', 'total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths', 'new_deaths_smoothed', 'total_cases_per_million', 'new_cases_per_million', 'new_cases_smoothed_per_million', 'total_deaths_per_million', 'new_deaths_per_million', 'new_deaths_smoothed_per_million','new_tests', 'total_tests', 'total_tests_per_thousand', 'new_tests_per_thousand', 'new_tests_smoothed', 'new_tests_smoothed_per_thousand', 'positive_rate', 'tests_per_case', 'tests_units', 'total_vaccinations', 'people_vaccinated', 'people_fully_vaccinated', 'new_vaccinations', 'new_vaccinations_smoothed', 'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred', 'new_vaccinations_smoothed_per_million'], axis = 1)\n",
    "#     info_full_df\n",
    "    vaccinations_full_df = vaccinations_full_df.join(info_full_df, how='outer').interpolate()\n",
    "    printProgressBar(100, 100, prefix = 'Tests and Vaccination data          ', suffix = 'Complete', length = 100)\n",
    "    printProgressBar(0, 100, prefix = 'Gonvernment Strategy data           ', suffix = 'Complete', length = 100)\n",
    "    \n",
    "    vector = [\"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c1_school_closing.csv\",\n",
    "          \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c2_workplace_closing.csv\"\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c3_cancel_public_events.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c4_restrictions_on_gatherings.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c5_close_public_transport.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c6_stay_at_home_requirements.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c7_movementrestrictions.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c8_internationaltravel.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/containment_health_index.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/e1_income_support.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/government_response_index.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/h2_testing_policy.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/h6_facial_coverings.csv'\n",
    "    , 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/stringency_index.csv']\n",
    "\n",
    "    vector_names = [\"school_closing\",\n",
    "              \"workplace_closing\"\n",
    "    , 'cancel_public_events'\n",
    "    , 'restrictions_on_gatherings'\n",
    "    , 'close_public_transport'\n",
    "    , 'stay_at_home_requirements'\n",
    "    , 'movementrestrictions'\n",
    "    , 'internationaltravel'\n",
    "    , 'containment_health_index'\n",
    "    , 'income_support'\n",
    "    , 'government_response_index'\n",
    "    , 'testing_policy'\n",
    "    , 'facial_coverings'\n",
    "    , 'stringency_index_2']\n",
    "\n",
    "    \n",
    "    gov_index = pd.DataFrame()\n",
    "    for i in range(len(vector)):\n",
    "        new = pd.read_csv(vector[i])\n",
    "        new = new[new[\"country_name\"] == cntry]\n",
    "        new  = new.drop(labels = ['country_name','country_code','Unnamed: 0'], axis = 1)\n",
    "        new = new.T\n",
    "        new.index = pd.to_datetime(new.index)\n",
    "        new.columns = [vector_names[i]]\n",
    "        gov_index = gov_index.join(new, how='outer').interpolate()\n",
    "        printProgressBar(i*5, 100, prefix = 'Gonvernment Strategy data           ', suffix = 'Complete', length = 100)\n",
    "    vaccinations_full_df = vaccinations_full_df.join(gov_index, how='outer').interpolate()\n",
    "    \n",
    "    printProgressBar(100, 100, prefix = 'Gonvernment Strategy data           ', suffix = 'Complete', length = 100)\n",
    "    \n",
    "    def MonthDayFormat(m):\n",
    "        if m <= 9:\n",
    "            return '0{}'.format(m)\n",
    "        else:\n",
    "            return m\n",
    "\n",
    "    x = datetime.datetime.now()\n",
    "\n",
    "    date_rng = pd.date_range(start='2020-03-22', end='{}-{}-{}'.format(x.year,MonthDayFormat(x.month),MonthDayFormat(x.day)), freq='D')\n",
    "    links = []\n",
    "    for i in range(date_rng.shape[0]):\n",
    "        links.append('https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/dailies/{}-{}-{}/{}-{}-{}_top1000terms.csv'.format(date_rng[i].year,MonthDayFormat(date_rng[i].month),MonthDayFormat(date_rng[i].day),date_rng[i].year,MonthDayFormat(date_rng[i].month),MonthDayFormat(date_rng[i].day)))\n",
    "\n",
    "\n",
    "\n",
    "    l = date_rng.shape[0]\n",
    "    printProgressBar(0, l, prefix = 'Twitter data                        ', suffix = 'Complete', length = 100)    \n",
    "    terms_usage_df = pd.DataFrame()\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    temp3 = []\n",
    "    temp4 = []\n",
    "    temp5 = []\n",
    "    temp6 = []\n",
    "    temp7 = []\n",
    "    temp8 = []\n",
    "    temp9 = []\n",
    "    temp10 = []\n",
    "    for i in range(date_rng.shape[0]):\n",
    "        try:\n",
    "            if links[i] == \"https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/dailies/2021-12-20/2021-12-20_top1000terms.csv\":\n",
    "                temp1.append(np.nan)\n",
    "                temp2.append(np.nan)\n",
    "                temp3.append(np.nan)\n",
    "                temp4.append(np.nan)\n",
    "                temp5.append(np.nan)\n",
    "                temp6.append(np.nan)\n",
    "                temp7.append(np.nan)\n",
    "                temp8.append(np.nan)\n",
    "                temp9.append(np.nan)\n",
    "                temp10.append(np.nan)\n",
    "                continue\n",
    "            temp_df = pd.read_csv(links[i],header=None)\n",
    "            printProgressBar(i + 1, l, prefix = 'Twitter data                        ', suffix = 'Complete', length = 100)\n",
    "        except:\n",
    "            printProgressBar(date_rng.shape[0], l, prefix = 'Twitter data                        ', suffix = 'Complete', length = 100)\n",
    "            date_rng = date_rng[:i]\n",
    "            break\n",
    "        temp_df.index = temp_df[0]\n",
    "        temp_df.drop([0], axis = 1, inplace = True)\n",
    "        temp_df = temp_df[~temp_df.index.duplicated(keep='first')]#drop duplicat-indexed rows\n",
    "        try:\n",
    "            temp1.append(temp_df[1]['coronavirus'])\n",
    "        except:\n",
    "            temp1.append(0)\n",
    "        try:\n",
    "            temp2.append(temp_df[1]['covid'])\n",
    "        except:\n",
    "            temp2.append(0)\n",
    "        try:\n",
    "            temp3.append(temp_df[1]['covid19'])\n",
    "        except:\n",
    "            temp3.append(0)\n",
    "        try:\n",
    "            temp4.append(temp_df[1]['lockdown'])\n",
    "        except:\n",
    "            temp4.append(0)\n",
    "        try:\n",
    "            temp5.append(temp_df[1]['coronavirus'])\n",
    "        except:\n",
    "            temp5.append(0)\n",
    "        try:\n",
    "            temp6.append(temp_df[1]['cases'])\n",
    "        except:\n",
    "            temp6.append(0)\n",
    "        try:\n",
    "            temp7.append(temp_df[1]['mask'])\n",
    "        except:\n",
    "            temp7.append(0)\n",
    "        try:\n",
    "            temp8.append(temp_df[1]['deaths'])\n",
    "        except:\n",
    "            temp8.append(0)\n",
    "        try:\n",
    "            temp9.append(temp_df[1]['quarantine'])\n",
    "        except:\n",
    "            temp9.append(0)\n",
    "        try:\n",
    "            temp10.append(temp_df[1]['virus'])\n",
    "        except:\n",
    "            temp10.append(0)\n",
    "    terms_usage_df['coronavirus'] = temp1\n",
    "    terms_usage_df['covid'] = temp2\n",
    "    terms_usage_df['covid19'] = temp3\n",
    "    terms_usage_df['lockdown'] = temp4\n",
    "    terms_usage_df['cases'] = temp5\n",
    "    terms_usage_df['pandemic'] = temp6\n",
    "    terms_usage_df['mask'] = temp7\n",
    "    terms_usage_df['deaths'] = temp8\n",
    "    terms_usage_df['quarantine'] = temp9\n",
    "    terms_usage_df['virus'] = temp10\n",
    "    terms_usage_df.index = date_rng\n",
    "    terms_usage_df.index = pd.to_datetime(terms_usage_df.index)\n",
    "\n",
    "    printProgressBar(0, 100, prefix = 'Twitter data manipulation           ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    printProgressBar(40, 100, prefix = 'Twitter data manipulation           ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(14):\n",
    "        temp.append(np.nan)\n",
    "    for i in range(terms_usage_df.index.shape[0]-14):\n",
    "        temp.append(terms_usage_df.iloc[i:7+i,0:9]['coronavirus'].sum()/7)\n",
    "    terms_usage_df['14_Day_Week_Average_coronavirus'] = temp\n",
    "    printProgressBar(50, 100, prefix = 'Twitter data manipulation           ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(14):\n",
    "        temp.append(np.nan)\n",
    "    for i in range(terms_usage_df.index.shape[0]-14):\n",
    "        temp.append(terms_usage_df.iloc[i:7+i,0:9]['lockdown'].sum()/7)\n",
    "    terms_usage_df['14_Day_Week_Average_lockdown'] = temp\n",
    "    printProgressBar(60, 100, prefix = 'Twitter data manipulation           ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(14):\n",
    "        temp.append(np.nan)\n",
    "    for i in range(terms_usage_df.index.shape[0]-14):\n",
    "        temp.append(terms_usage_df.iloc[i:7+i,0:9]['cases'].sum()/7)\n",
    "    terms_usage_df['14_Day_Week_Average_cases'] = temp\n",
    "    printProgressBar(70, 100, prefix = 'Twitter data manipulation           ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(14):\n",
    "        temp.append(np.nan)\n",
    "    for i in range(terms_usage_df.index.shape[0]-14):\n",
    "        temp.append(terms_usage_df.iloc[i:7+i,0:9]['mask'].sum()/7)\n",
    "    terms_usage_df['14_Day_Week_Average_mask'] = temp\n",
    "    printProgressBar(80, 100, prefix = 'Twitter data manipulation           ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(14):\n",
    "        temp.append(np.nan)\n",
    "    for i in range(terms_usage_df.index.shape[0]-14):\n",
    "        temp.append(terms_usage_df.iloc[i:7+i,0:9]['pandemic'].sum()/7)\n",
    "    terms_usage_df['14_Day_Week_Average_pandemic'] = temp\n",
    "    printProgressBar(90, 100, prefix = 'Twitter data manipulation           ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(14):\n",
    "        temp.append(np.nan)\n",
    "    for i in range(terms_usage_df.index.shape[0]-14):\n",
    "        temp.append(terms_usage_df.iloc[i:7+i,0:9]['deaths'].sum()/7)\n",
    "    terms_usage_df['14_Day_Week_Average_deaths'] = temp\n",
    "    printProgressBar(100, 100, prefix = 'Twitter data manipulation           ', suffix = 'Complete', length = 100)\n",
    "    # terms_usage_df\n",
    "\n",
    "    printProgressBar(0, 100, prefix = 'Merging Datasets                    ', suffix = 'Complete', length = 100)\n",
    "\n",
    "    full_dataset = vaccinations_full_df.join(terms_usage_df, how='outer').interpolate()\n",
    "    printProgressBar(100, 100, prefix = 'Merging Datasets                    ', suffix = 'Complete', length = 100)\n",
    "    \n",
    "    if(saveType != 'variable'):\n",
    "        if(saveType == 'csv'):\n",
    "            if(savePath == \"/\"):\n",
    "                print('No path provided! Saving to working directory with name {}'.format(f'{cntry}.csv'))\n",
    "                full_dataset.to_csv(f'{cntry}.csv', index = True, header=True)\n",
    "            else:\n",
    "                full_dataset.to_csv(savePath, index = True, header=True)\n",
    "        elif(saveType == 'excel'):\n",
    "            if(savePath == \"/\"):\n",
    "                print('No path provided! Saving to working directory with name {}'.format(f'{cntry}.csv'))\n",
    "                full_dataset.to_excel(f'{cntry}.csv')\n",
    "            else:\n",
    "                full_dataset.to_excel(savePath)\n",
    "    print(\"---Execution finished in %s seconds ---\" % (time.time() - start_time))\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ce8a73c-e648-4cfd-ba26-6fc96800288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed, Deaths and Recovered data |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "Preparing data                       |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "Calculating Daily data               |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "Gathering Weather data               |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "Tests and Vaccination data           |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "Gonvernment Strategy data            |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "Twitter data                         |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "Twitter data manipulation            |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "Merging Datasets                     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "---Execution finished in 290.79341101646423 seconds ---\n"
     ]
    }
   ],
   "source": [
    "UK_df = createCovidDataFrame(\"United Kingdom\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
